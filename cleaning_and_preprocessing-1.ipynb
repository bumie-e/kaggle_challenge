{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Important\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "sam = pd.read_csv('sample_submission2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeNo</th>\n",
       "      <th>Division</th>\n",
       "      <th>Qualification</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Channel_of_Recruitment</th>\n",
       "      <th>Trainings_Attended</th>\n",
       "      <th>Year_of_birth</th>\n",
       "      <th>Last_performance_score</th>\n",
       "      <th>Year_of_recruitment</th>\n",
       "      <th>Targets_met</th>\n",
       "      <th>Previous_Award</th>\n",
       "      <th>Training_score_average</th>\n",
       "      <th>State_Of_Origin</th>\n",
       "      <th>Foreign_schooled</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Past_Disciplinary_Action</th>\n",
       "      <th>Previous_IntraDepartmental_Movement</th>\n",
       "      <th>No_of_previous_employers</th>\n",
       "      <th>Promoted_or_Not</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26674</th>\n",
       "      <td>YAK/S/38053</td>\n",
       "      <td>Sourcing and Purchasing</td>\n",
       "      <td>First Degree or HND</td>\n",
       "      <td>Female</td>\n",
       "      <td>Direct Internal process</td>\n",
       "      <td>4</td>\n",
       "      <td>1991</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>RIVERS</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Married</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        EmployeeNo                 Division        Qualification  Gender  \\\n",
       "26674  YAK/S/38053  Sourcing and Purchasing  First Degree or HND  Female   \n",
       "\n",
       "        Channel_of_Recruitment  Trainings_Attended  Year_of_birth  \\\n",
       "26674  Direct Internal process                   4           1991   \n",
       "\n",
       "       Last_performance_score  Year_of_recruitment  Targets_met  \\\n",
       "26674                    10.0                 2014            1   \n",
       "\n",
       "       Previous_Award  Training_score_average State_Of_Origin  \\\n",
       "26674               0                      65          RIVERS   \n",
       "\n",
       "      Foreign_schooled Marital_Status Past_Disciplinary_Action  \\\n",
       "26674              Yes        Married                       No   \n",
       "\n",
       "      Previous_IntraDepartmental_Movement No_of_previous_employers  \\\n",
       "26674                                  No                        1   \n",
       "\n",
       "       Promoted_or_Not  \n",
       "26674                0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain = pd.get_dummies(train, columns = ['Division', 'Gender'])\n",
    "itest = pd.get_dummies(test, columns = ['Division', 'Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in itrain.Year_of_recruitment:\n",
    "    if i in itest.Year_of_recruitment:\n",
    "        pass\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1991    2544\n",
       "1990    2506\n",
       "1989    2441\n",
       "1992    2380\n",
       "1988    2268\n",
       "1993    2217\n",
       "1987    2157\n",
       "1994    1985\n",
       "1986    1882\n",
       "1985    1766\n",
       "1984    1505\n",
       "1995    1416\n",
       "1983    1342\n",
       "1982    1195\n",
       "1981    1162\n",
       "1996     910\n",
       "1980     893\n",
       "1979     797\n",
       "1978     685\n",
       "1977     600\n",
       "1997     596\n",
       "1976     551\n",
       "1975     508\n",
       "1973     397\n",
       "1974     394\n",
       "1971     353\n",
       "1972     312\n",
       "1998     305\n",
       "1970     256\n",
       "1969     249\n",
       "1968     233\n",
       "1966     207\n",
       "1967     195\n",
       "1965     178\n",
       "1964     168\n",
       "1999     167\n",
       "1963     161\n",
       "1961     147\n",
       "1962     134\n",
       "2001      71\n",
       "2000      66\n",
       "1957       4\n",
       "1958       3\n",
       "1960       1\n",
       "1959       1\n",
       "1956       1\n",
       "1955       1\n",
       "1950       1\n",
       "1952       1\n",
       "Name: Year_of_birth, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itrain.Year_of_birth.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.drop(['Year_of_birth', 'State_Of_Origin', 'Marital_Status', 'Channel_of_Recruitment', 'Previous_IntraDepartmental_Movement'], axis=1, inplace=True)\n",
    "itest.drop(['Year_of_birth', 'State_Of_Origin', 'Marital_Status', 'Channel_of_Recruitment', 'Previous_IntraDepartmental_Movement'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.shape, itest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Qualification.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain['Qualification'][itrain['Qualification'] == 'MSc, MBA and PhD'] = 0\n",
    "itrain['Qualification'][itrain['Qualification'] == 'First Degree or HND'] = 1\n",
    "itrain['Qualification'][itrain['Qualification'] == 'Non-University Education'] = 2\n",
    "\n",
    "\n",
    "itest['Qualification'][itest['Qualification'] == 'MSc, MBA and PhD'] = 0\n",
    "itest['Qualification'][itest['Qualification'] == 'First Degree or HND'] = 1\n",
    "itest['Qualification'][itest['Qualification'] == 'Non-University Education'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Trainings_Attended.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 11] = 0\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 10] = 1\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 9] = 2\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 8] = 3\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 7] = 4\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 6] = 5\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 5] = 6\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 4] = 7\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 3] = 8\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 2] = 9\n",
    "\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 11] = 0\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 10] = 1\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 9] = 2\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 8] = 3\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 7] = 4\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 6] = 5\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 5] = 6\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 4] = 7\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 3] = 8\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 2] = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Targets_met.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain['Targets_met'][itrain['Targets_met'] == 1] = 0\n",
    "itrain['Targets_met'][itrain['Targets_met'] == 0] = 1\n",
    "\n",
    "itest['Targets_met'][itest['Targets_met'] == 1] = 0\n",
    "itest['Targets_met'][itest['Targets_met'] == 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Previous_Award.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain['Previous_Award'][itrain['Previous_Award'] == 1] = 0\n",
    "itrain['Previous_Award'][itrain['Previous_Award'] == 0] = 1\n",
    "\n",
    "itest['Previous_Award'][itest['Previous_Award'] == 1] = 0\n",
    "itest['Previous_Award'][itest['Previous_Award'] == 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 'More than 5'] = 0\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 5] = 1\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 4] = 2\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 3] = 3\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 2] = 4\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 1] = 5\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 0] = 6\n",
    "\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 'More than 5'] = 0\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 5] = 1\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 4] = 2\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 3] = 3\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 2] = 4\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 1] = 5\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 0] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Last_performance_score.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 12.5] = 0\n",
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 10.0] = 1\n",
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 7.5] = 2\n",
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 5.0] = 3\n",
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 2.5] = 4\n",
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 0.0] = 5\n",
    "\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 12.5] = 0\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 10.0] = 1\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 7.5] = 2\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 5.0] = 3\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 2.5] = 4\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 0.0] = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itest.Last_performance_score.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in itest.Year_of_recruitment:\n",
    "    if i in itrain.Year_of_recruitment:\n",
    "        pass\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## itrain = pd.get_dummies(itrain, columns=['Year_of_recruitment'])\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1982] = 0\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1985] = 1\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1986] = 2\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1987] = 3\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1988] = 4\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1989] = 5\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1990] = 6\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1991] = 7\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1992] = 8\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1993] = 9\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1994] = 10\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1995] = 11\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1996] = 12\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1997] = 13\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1998] = 14\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1999] = 15\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2000] = 16\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2001] = 17\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2002] = 18\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2003] = 19\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2004] = 20\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2005] = 21\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2006] = 22\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2007] = 23\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2008] = 24\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2009] = 25\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2010] = 26\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2011] = 27\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2012] = 28\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2013] = 29\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2014] = 30\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2015] = 31\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2016] = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#itrain = pd.get_dummies(itrain, columns=['Year_of_recruitment'])\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1982] = 0\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1985] = 1\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1986] = 2\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1987] = 3\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1988] = 4\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1989] = 5\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1990] = 6\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1991] = 7\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1992] = 8\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1993] = 9\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1994] = 10\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1995] = 11\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1996] = 12\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1997] = 13\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1998] = 14\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1999] = 15\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2000] = 16\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2001] = 17\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2002] = 18\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2003] = 19\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2004] = 20\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2005] = 21\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2006] = 22\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2007] = 23\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2008] = 24\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2009] = 25\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2010] = 26\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2011] = 27\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2012] = 28\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2013] = 29\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2014] = 30\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2015] = 31\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2016] = 32\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2017] = 33\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2018] = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"m = itrain['Year_of_recruitment']\n",
    "for i in m:\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(itrain[i].values))\n",
    "    itrain[i] = lbl.transform(list(itrain[i].values))\n",
    "\n",
    "print('Shape all_data : {}'.format(itrain.shape))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for i in itrain.Year_of_recruitment:\n",
    "    if i in c:\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(itrain[i].values))\n",
    "        itrain[i] = lbl.transform(list(itrain[i].values))\n",
    "    else:\n",
    "        pass\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Foreign_schooled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Foreign_schooled[itrain.Foreign_schooled == 'Yes'] = 0\n",
    "itrain.Foreign_schooled[itrain.Foreign_schooled == 'No'] = 1\n",
    "\n",
    "itest.Foreign_schooled[itest.Foreign_schooled == 'Yes'] = 0\n",
    "itest.Foreign_schooled[itest.Foreign_schooled == 'No'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Past_Disciplinary_Action.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Past_Disciplinary_Action[itrain.Past_Disciplinary_Action == 'No'] = 0\n",
    "itrain.Past_Disciplinary_Action[itrain.Past_Disciplinary_Action == 'Yes'] = 1\n",
    "\n",
    "itest.Past_Disciplinary_Action[itest.Past_Disciplinary_Action == 'No'] = 0\n",
    "itest.Past_Disciplinary_Action[itest.Past_Disciplinary_Action == 'Yes'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Previous_IntraDepartmental_Movement.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#itrain = pd.get_dummies(itrain, columns=['Previous_IntraDepartmental_Movement'])\n",
    "\n",
    "#itest = pd.get_dummies(itest, columns=['Previous_IntraDepartmental_Movement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Qualification = pd.to_numeric(itrain.Qualification)\n",
    "itrain.Foreign_schooled = pd.to_numeric(itrain.Foreign_schooled)\n",
    "itrain.Past_Disciplinary_Action = pd.to_numeric(itrain.Past_Disciplinary_Action)\n",
    "itrain.No_of_previous_employers = pd.to_numeric(itrain.No_of_previous_employers)\n",
    "\n",
    "itest.Qualification = pd.to_numeric(itest.Qualification)\n",
    "itest.Foreign_schooled = pd.to_numeric(itest.Foreign_schooled)\n",
    "itest.Past_Disciplinary_Action = pd.to_numeric(itest.Past_Disciplinary_Action)\n",
    "itest.No_of_previous_employers = pd.to_numeric(itest.No_of_previous_employers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38312 entries, 0 to 38311\n",
      "Data columns (total 23 columns):\n",
      "EmployeeNo                                              38312 non-null object\n",
      "Qualification                                           36633 non-null float64\n",
      "Trainings_Attended                                      38312 non-null int64\n",
      "Last_performance_score                                  38312 non-null float64\n",
      "Year_of_recruitment                                     38312 non-null int64\n",
      "Targets_met                                             38312 non-null int64\n",
      "Previous_Award                                          38312 non-null int64\n",
      "Training_score_average                                  38312 non-null int64\n",
      "Foreign_schooled                                        38312 non-null int64\n",
      "Past_Disciplinary_Action                                38312 non-null int64\n",
      "No_of_previous_employers                                38312 non-null int64\n",
      "Promoted_or_Not                                         38312 non-null int64\n",
      "Division_Business Finance Operations                    38312 non-null uint8\n",
      "Division_Commercial Sales and Marketing                 38312 non-null uint8\n",
      "Division_Customer Support and Field Operations          38312 non-null uint8\n",
      "Division_Information Technology and Solution Support    38312 non-null uint8\n",
      "Division_Information and Strategy                       38312 non-null uint8\n",
      "Division_People/HR Management                           38312 non-null uint8\n",
      "Division_Regulatory and Legal services                  38312 non-null uint8\n",
      "Division_Research and Innovation                        38312 non-null uint8\n",
      "Division_Sourcing and Purchasing                        38312 non-null uint8\n",
      "Gender_Female                                           38312 non-null uint8\n",
      "Gender_Male                                             38312 non-null uint8\n",
      "dtypes: float64(2), int64(9), object(1), uint8(11)\n",
      "memory usage: 3.9+ MB\n"
     ]
    }
   ],
   "source": [
    "itrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_Id = itrain.EmployeeNo\n",
    "itrain.drop(['EmployeeNo'], axis=1, inplace=True)\n",
    "itest.drop(['EmployeeNo'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in itrain.columns:\n",
    "    itrain[i].fillna(itrain[i].mode()[0], inplace = True)\n",
    "    \n",
    "for i in itest.columns:\n",
    "    itest[i].fillna(itest[i].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scaling And Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = itrain.Promoted_or_Not\n",
    "itrain.drop(['Promoted_or_Not'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "std = Normalizer()\n",
    "std_itrain = std.fit_transform(itrain)\n",
    "std_itest = std.fit_transform(itest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''x = itrain[itrain.Promoted_or_Not]\n",
    "y = itrain[itrain.Promoted_or_Not]\n",
    "\n",
    "y = np.where((y == 0), 0, 1)\n",
    "\n",
    "i_class0 = np.where(itrain.Promoted_or_Not == 0)[0]\n",
    "i_class1 = np.where(itrain.Promoted_or_Not == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def using_smote(X, y):\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    sm = SMOTE()\n",
    "    X, y = sm.fit_sample(X, y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val, target_val = using_smote(itrain, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#itrain.drop(['Trainings_Attended_10'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train,y_test= train_test_split(itrain, target, test_size = 0.22, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29883, 21), (8429, 21))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm #support vector Machine\n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest\n",
    "from sklearn.neighbors import KNeighborsClassifier #KNN\n",
    "from sklearn.naive_bayes import GaussianNB #Naive bayes\n",
    "from sklearn.tree import DecisionTreeClassifier #Decision Tree\n",
    "from sklearn.model_selection import train_test_split #training and testing data split\n",
    "from sklearn import metrics #accuracy measure\n",
    "from sklearn.metrics import confusion_matrix #for confusion matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9253756316300238, 0.9227666389844584)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(learning_rate = 0.2, n_estimators = 270, subsample=0.9, max_features = 'sqrt', max_depth = 1, random_state=7)\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_train, y_train), model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38312, 21), (16496, 21))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itrain.shape, itest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9227666389844584"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred3 = model.predict(x_test)\n",
    "pred2 = model.predict(itest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20318237454100366"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(pred3, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, performCV=True, printFeatureImportance=True, cv_folds=5):\n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(x_train, y_train)\n",
    "        \n",
    "    #Predict training set:\n",
    "    pred = alg.predict(x_test)\n",
    "    predprob = alg.predict_proba(x_test)\n",
    "    \n",
    "    #Perform cross-validation:\n",
    "    if performCV:\n",
    "        cv_score = cross_validation.cross_val_score(alg, x_train, y_train, cv=cv_folds, scoring='roc_auc')\n",
    "    \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_test, pred))\n",
    "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(y_test, predprob))\n",
    "    \n",
    "    if performCV:\n",
    "        print(\"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n",
    "        \n",
    "    #Print Feature Importance:\n",
    "    if printFeatureImportance:\n",
    "        feat_imp = pd.Series(alg.feature_importances_, train['Promoted_or_Not']).sort_values(ascending=False)\n",
    "        feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cross_validation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-2acbd2d66ab4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0malg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodelfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-36-400984ac702e>\u001b[0m in \u001b[0;36mmodelfit\u001b[1;34m(model, performCV, printFeatureImportance, cv_folds)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#Perform cross-validation:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mperformCV\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mcv_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv_folds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'roc_auc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m#Print model report:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cross_validation' is not defined"
     ]
    }
   ],
   "source": [
    "alg = model\n",
    "modelfit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "çlassification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96      8332\n",
      "           1       0.12      0.86      0.20        97\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      8429\n",
      "   macro avg       0.56      0.89      0.58      8429\n",
      "weighted avg       0.99      0.92      0.95      8429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see the overall performance of the data\n",
    "from sklearn.metrics import classification_report\n",
    "print('çlassification_report')\n",
    "print(classification_report(pred3, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8242917199780662, 0.8240668740279938)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(x_train, y_train)\n",
    "xgb.score(x_train, y_train), xgb.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8089239214582308"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred2 = xgb.predict(x_test)\n",
    "# get the f1 score\n",
    "f1_score(pred2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using the itest data\n",
    "pred2 = xgb.predict(itest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data = {'EmployeeNo':target_Id, 'Promoted_or_Not':pred})\n",
    "output.to_csv(path_or_buf = 'xgbbumiepredi.csv', index = False, quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "lr.score(x_train, y_train), lr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.shape, itest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(itest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(pred1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_Id = test.EmployeeNo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = ['l1', 'l2']\n",
    "C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "class_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\n",
    "solver = ['liblinear', 'saga']\n",
    "\n",
    "param_grid = dict(penalty=penalty,\n",
    "                  C=C,\n",
    "                  class_weight=class_weight,\n",
    "                  solver=solver)\n",
    "\n",
    "grid = GridSearchCV(estimator=lr,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring='roc_auc',\n",
    "                    verbose=1,\n",
    "                    n_jobs=-1)\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "\n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.score(x_train, y_train),grid.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = grid.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = grid.predict(itest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbrt = GradientBoostingClassifier()\n",
    "learning_rate = [0.2,0.5,0.7]\n",
    "\n",
    "n_estimators = [100,200,270,400]\n",
    "\n",
    "param_grid = dict(learning_rate = learning_rate,\n",
    "                 n_estimators = n_estimators)\n",
    "\n",
    "grid = GridSearchCV(estimator=gbrt,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring='roc_auc',\n",
    "                    verbose=1,\n",
    "                    n_jobs=-1)\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "\n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = grid.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = grid.predict(itest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(pred1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data = {'EmployeeNo':target_Id, 'Promoted_or_Not':pred2})\n",
    "output.to_csv(path_or_buf = 'gbbumiepredi.csv', index = False, quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.fit(x_train, y_train)\n",
    "xgb.score(x_train, y_train), model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vecstack'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-da5badb758dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mvecstack\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstacking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'vecstack'"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
