{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Name : Akinremi Bunmi Josephine.\n",
    "\n",
    "<h3>kaggle_Id : bumie.\n",
    "\n",
    "<h3>Student_Status: 100 level.\n",
    "\n",
    "<h3>Gender : Female.\n",
    "\n",
    "<h3>Special_Services : AI+ Club Campus Co-ordinator\n",
    "\n",
    "<h3>Name_of_School : Obafemi Awolowo University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Important\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import the testdata, train data, submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "sam = pd.read_csv('sample_submission2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lets take a brief look at the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> We can see that the train dataset has 38312 rows and 19 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Lets take a brief look at the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> We can see that the test dataset has 16496 columns and 18 rows</b>\n",
    "\n",
    "<h3> Does our dataset contain any null value? Lets find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Hmmn, we can see that the column \"Qualification\" has only 36633 of its values that are not null. \n",
    "<h3> We can also see the various datatypes present in the dataset.(We'll deal with these later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lets divide the dataset into biased and unbiased. \n",
    "    \n",
    "\n",
    "<h3>Lets first take a look at the unbiased dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>The Unbiased dataset contains the following columns: Qualification, Target_met, Previous_Award, Trainning_Score_Average,\n",
    "<h4>Foregin_Schooled, Past_Disiciplanry_Action, Previous_Intradepartmental_Movement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> First of all, lets create a dataframe containing the test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain = train\n",
    "itest = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lets Review the Qualification Column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Qualification.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>For better view let's plot these columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(x, y):\n",
    "    fig, ax = plt.subplots(figsize = (13, 4))\n",
    "    sns.barplot(x, y)\n",
    "    plt.xlabel('x', fontSize=13)\n",
    "    plt.xticks(rotation='90')\n",
    "    plt.ylabel('Promoted_or_Not', fontSize=13)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test.Qualification\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
    "train['Qualification'].value_counts().plot.pie(explode=[0,0.1,0.2],autopct='%1.1f%%',ax=ax[0], shadow=True)\n",
    "ax[0].set_title('Qualification vs promotion')\n",
    "ax[0].set_ylabel('Qualification')\n",
    "sns.factorplot('Qualification', 'Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Qualification vs promotion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h4> From the above, we can say that more of the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Review the Target_met column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Targets_met.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Targets_met','Promoted_or_Not',hue='Targets_met',data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
    "train[['Targets_met','Promoted_or_Not']].groupby(['Targets_met']).mean().plot.bar(ax=ax[0])\n",
    "ax[0].set_title('Targets_met vs Promoted_or_Not')\n",
    "sns.countplot('Targets_met',hue='Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Targets_met:Promoted or Not')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test.Targets_met\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets review the Previous_Award column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Previous_Award.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Review the Promoted_or_Not Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better view let's plot these percents!\n",
    "x = test.Previous_Award\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "sns.barplot('Previous_Award','Promoted_or_Not',data=train,ax=ax[0])\n",
    "ax[0].set_title('Previous_Award vs Promoted_or_Not')\n",
    "sns.factorplot('Previous_Award','Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Previous_Award vs Promoted_or_Not')\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Previous_Award','Promoted_or_Not',hue='Previous_Award',data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets review the Trainning score average column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train.Training_score_average\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Training_score_average','Promoted_or_Not',data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets review the Foregin_Schooled column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Foreign_schooled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test.Foreign_schooled\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "sns.distplot(train[train['Foreign_schooled']=='Yes'].Promoted_or_Not,ax=ax[0])\n",
    "ax[0].set_title('Schooled Aborad:Yes')\n",
    "sns.distplot(train[train['Foreign_schooled']=='No'].Promoted_or_Not,ax=ax[1])\n",
    "ax[1].set_title('Schooled Abroad:No')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the Past_Disciplinary_Action column!,\n",
    "\n",
    "And see those who has been disciplined(bad boys) before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Past_Disciplinary_Action.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Past_Disciplinary_Action','Promoted_or_Not',data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test.Past_Disciplinary_Action\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "sns.distplot(train[train['Past_Disciplinary_Action']=='Yes'].Promoted_or_Not,ax=ax[0])\n",
    "ax[0].set_title('Past_Disciplinary_Action:Yes')\n",
    "sns.distplot(train[train['Past_Disciplinary_Action']=='No'].Promoted_or_Not,ax=ax[1])\n",
    "ax[1].set_title('Past_Disciplinary_Action:No')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the Previous_Intradepartmental_Movement column. Some might have even been promoted before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Previous_IntraDepartmental_Movement','Promoted_or_Not',data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = itrain.Previous_IntraDepartmental_Movement\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
    "train[['Previous_IntraDepartmental_Movement','Promoted_or_Not']].groupby(['Previous_IntraDepartmental_Movement']).mean().plot.bar(ax=ax[0])\n",
    "ax[0].set_title('Previous_IntraDepartmental_Movement vs Promoted_or_Not')\n",
    "sns.countplot('Previous_IntraDepartmental_Movement',hue='Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Previous_IntraDepartmental_Movement:Promoted or Not')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,15))\n",
    "sns.countplot('Previous_IntraDepartmental_Movement',data=train,ax=ax[0])\n",
    "ax[0].set_title('Previous_IntraDepartmental_Movement')\n",
    "sns.countplot('Previous_IntraDepartmental_Movement',hue='Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Previous_IntraDepartmental_Movement for Promoted_or_Not')\n",
    "plt.subplots_adjust(wspace=0.2,hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after we are done with that, lets take a look at the Biased dataset.\n",
    "\n",
    "<h4>The Biased dataset contains the following : </h4>\n",
    "Division, Marital_Status, Gender, Channel_of_Recruitment, Year_of_birth, Trainings_Attended, Last_performance_score, No_of_previous employers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train.Division\n",
    "y = train.Promoted_or_Not\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test.Division\n",
    "y = train.Promoted_or_Not\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Division', 'Promoted_or_Not',data=train)\n",
    "plt.xlabel('Division Columns', fontSize=13)\n",
    "plt.xticks(rotation='90')\n",
    "plt.ylabel('Promoted_or_Not', fontSize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(y,x,orient = 'h')\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
    "train['Division'].value_counts().plot.pie(explode=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8],autopct='%1.1f%%',ax=ax[0], shadow=True)\n",
    "ax[0].set_title('Division vs promotion')\n",
    "ax[0].set_ylabel('Division')\n",
    "sns.factorplot('Division', 'Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Division vs promotion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> From these, we can see that this column is biased as some divisions are more promoted than some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test.Gender\n",
    "y = train.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better view let's plot these percents!\n",
    "x = train.Gender\n",
    "y = train.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "train['Gender'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0], shadow=True)\n",
    "ax[0].set_title('Gender vs Promoted_or_Not')\n",
    "sns.factorplot('Gender','Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Gender vs Promoted_or_Not')\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> From here, we can see that there are more males than females but, more females are promoted than males!\n",
    "    \n",
    "<h3> Lets look at the Channel_of_Recruitment column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Channel_of_Recruitment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test.Channel_of_Recruitment\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better view let's plot these percents!\n",
    "x = train.Channel_of_Recruitment\n",
    "y = train.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "sns.barplot('Channel_of_Recruitment','Promoted_or_Not',data=train,ax=ax[0])\n",
    "ax[0].set_title('Channel_of_Recruitment vs Promoted_or_Not')\n",
    "sns.factorplot('Channel_of_Recruitment','Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Channel_of_Recruitment vs Promoted_or_Not')\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Am sure you'll agree that with me that those who had special referral even got promoted than those who cme through the normal qualified way\n",
    "    \n",
    "<h3> So now, Lets take a look at the Trainings_Attended column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Trainings_Attended.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train.Trainings_Attended\n",
    "y = train.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(y,x,orient = 'h')\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "sns.barplot('Trainings_Attended','Promoted_or_Not',data=train,ax=ax[0])\n",
    "ax[0].set_title('Trainings_Attended vs Promoted_or_Not')\n",
    "sns.factorplot('Trainings_Attended','Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Trainings_Attended vs Promoted_or_Not')\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Well, its just obivious that these employees were not promoted based on their performance\n",
    "\n",
    "<h3> Without much ado, lets move to the Year_of_birth column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Year_of_birth.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better view let's plot these percents!\n",
    "x = train.Year_of_birth\n",
    "y = train.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "sns.barplot('Year_of_birth','Promoted_or_Not',data=train,ax=ax[0])\n",
    "ax[0].set_title('Year_of_birth vs Promoted_or_Not')\n",
    "sns.factorplot('Year_of_birth','Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Year_of_birth vs Promoted_or_Not')\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.barplot(y,x,orient = 'h')\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> From these, we can really tell whose who because they are ll closely packed together\n",
    "\n",
    "<h3> Now, moving on to Last_performance_score column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Last_performance_score.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test.Last_performance_score\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better view let's plot these percents!\n",
    "x = itrain.Last_performance_score\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(y,x,orient = 'h')\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'NaN' in itrain.Last_performance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "sns.barplot('Last_performance_score','Promoted_or_Not',data=train,ax=ax[0])\n",
    "ax[0].set_title('Last_performance_score vs Promoted_or_Not')\n",
    "sns.factorplot('Last_performance_score','Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Last_performance_score vs Promoted_or_Not')\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> We can also see some imbalances especially with those who scored 0.0 points\n",
    "\n",
    "<h3>Ok, lets look at the Year_of_recruitment column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Year_of_recruitment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test.Year_of_recruitment\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = itrain.Year_of_recruitment\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Year_of_recruitment','Promoted_or_Not',hue='Promoted_or_Not',data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
    "train[['Year_of_recruitment','Promoted_or_Not']].groupby(['Year_of_recruitment']).mean().plot.bar(ax=ax[0])\n",
    "ax[0].set_title('Promoted_or_Not vs Year_of_recruitment')\n",
    "sns.countplot('Year_of_recruitment',hue='Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Year_of_recruitment:Promoted or Not')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "sns.barplot('Year_of_recruitment','Promoted_or_Not',data=train,ax=ax[0])\n",
    "ax[0].set_title('Year_of_recruitment vs Promoted_or_Not')\n",
    "sns.factorplot('Year_of_recruitment','Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Year_of_recruitment vs Promoted_or_Not')\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> We can also see imbalances here too with this column\n",
    "    \n",
    "<h3> Next! Taking a look at the State_Of_Origin column,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.State_Of_Origin.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better view let's plot these percents!\n",
    "x = itrain.State_Of_Origin\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test.State_Of_Origin\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('State_Of_Origin','Promoted_or_Not',hue='State_Of_Origin',data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "sns.barplot('State_Of_Origin','Promoted_or_Not',data=train,ax=ax[0])\n",
    "ax[0].set_title('State_Of_Origin vs Promoted_or_Not')\n",
    "sns.factorplot('State_Of_Origin','Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('State_Of_Origin vs Promoted_or_Not')\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Also in this column, we can see various irregularities in the pattern\n",
    "\n",
    "\n",
    "<h3> Now, lets look at the Foreign_schooled column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Marital_Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "sns.barplot('Marital_Status','Promoted_or_Not',data=train,ax=ax[0])\n",
    "ax[0].set_title('Marital_Status vs Promoted_or_Not')\n",
    "sns.factorplot('Marital_Status','Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Marital_Status vs Promoted_or_Not')\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test.Marital_Status\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Marital_Status','Promoted_or_Not',hue='Marital_Status',data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train.Marital_Status\n",
    "y = train.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.No_of_previous_employers.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better view let's plot these percents!\n",
    "x = itrain.No_of_previous_employers\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('No_of_previous_employers','Promoted_or_Not',hue='No_of_previous_employers',data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
    "itrain['No_of_previous_employers'].value_counts().plot.pie(explode=[0,0.1,0.2,0.3,0.4,0.5,0.6],autopct='%1.1f%%',ax=ax[0], shadow=True)\n",
    "ax[0].set_title('No_of_previous_employers vs promotion')\n",
    "ax[0].set_ylabel('No_of_previous_employers')\n",
    "sns.factorplot('No_of_previous_employers', 'Promoted_or_Not',data=itrain,ax=ax[1])\n",
    "ax[1].set_title('No_of_previous_employers vs promotion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Here also, we can basically see the irregularity in thr promoting pattern\n",
    "    \n",
    "<h3> So now, we are basically done with visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Now, lets Label Encode and One Hot Encode our data, where neccesary and drop few unimportant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain = pd.get_dummies(train, columns = ['Channel_of_Recruitment','Marital_Status', 'Division', 'Previous_IntraDepartmental_Movement', 'State_Of_Origin', 'Year_of_birth', 'Gender'])\n",
    "itest = pd.get_dummies(test, columns = ['Channel_of_Recruitment', 'Marital_Status', 'Division', 'Previous_IntraDepartmental_Movement', 'State_Of_Origin', 'Year_of_birth', 'Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain['Qualification'][itrain['Qualification'] == 'MSc, MBA and PhD'] = 2\n",
    "itrain['Qualification'][itrain['Qualification'] == 'First Degree or HND'] = 0\n",
    "itrain['Qualification'][itrain['Qualification'] == 'Non-University Education'] = 1\n",
    "\n",
    "\n",
    "itest['Qualification'][itest['Qualification'] == 'MSc, MBA and PhD'] = 2\n",
    "itest['Qualification'][itest['Qualification'] == 'First Degree or HND'] = 0\n",
    "itest['Qualification'][itest['Qualification'] == 'Non-University Education'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Marital_Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain['Marital_Status'][itrain['Marital_Status'] == 'MSc, MBA and PhD'] = 2\n",
    "itrain['Marital_Status'][itrain['Marital_Status'] == 'First Degree or HND'] = 0\n",
    "itrain['Marital_Status'][itrain['Marital_Status'] == 'Non-University Education'] = 1\n",
    "\n",
    "\n",
    "itest['Marital_Status'][itest['Marital_Status'] == 'MSc, MBA and PhD'] = 2\n",
    "itest['Marital_Status'][itest['Marital_Status'] == 'First Degree or HND'] = 0\n",
    "itest['Marital_Status'][itest['Marital_Status'] == 'Non-University Education'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain['Targets_met'][itrain['Targets_met'] == 1] = 0\n",
    "itrain['Targets_met'][itrain['Targets_met'] == 0] = 1\n",
    "\n",
    "itest['Targets_met'][itest['Targets_met'] == 1] = 0\n",
    "itest['Targets_met'][itest['Targets_met'] == 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain['Previous_Award'][itrain['Previous_Award'] == 1] = 0\n",
    "itrain['Previous_Award'][itrain['Previous_Award'] == 0] = 1\n",
    "\n",
    "itest['Previous_Award'][itest['Previous_Award'] == 1] = 0\n",
    "itest['Previous_Award'][itest['Previous_Award'] == 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Foreign_schooled[itrain.Foreign_schooled == 'Yes'] = 0\n",
    "itrain.Foreign_schooled[itrain.Foreign_schooled == 'No'] = 1\n",
    "\n",
    "itest.Foreign_schooled[itest.Foreign_schooled == 'Yes'] = 0\n",
    "itest.Foreign_schooled[itest.Foreign_schooled == 'No'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Past_Disciplinary_Action[itrain.Past_Disciplinary_Action == 'No'] = 0\n",
    "itrain.Past_Disciplinary_Action[itrain.Past_Disciplinary_Action == 'Yes'] = 1\n",
    "\n",
    "itest.Past_Disciplinary_Action[itest.Past_Disciplinary_Action == 'No'] = 0\n",
    "itest.Past_Disciplinary_Action[itest.Past_Disciplinary_Action == 'Yes'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 12.5] = 0\n",
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 10.0] = 1\n",
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 7.5] = 2\n",
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 5.0] = 3\n",
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 2.5] = 4\n",
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 0.0] = 5\n",
    "\n",
    "\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 12.5] = 0\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 10.0] = 1\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 7.5] = 2\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 5.0] = 3\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 2.5] = 4\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 0.0] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 11] = 0\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 10] = 1\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 9] = 2\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 8] = 3\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 7] = 4\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 6] = 5\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 5] = 6\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 4] = 7\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 3] = 8\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 2] = 9\n",
    "\n",
    "\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 11] = 0\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 9] = 1\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 8] = 2\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 7] = 3\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 6] = 4\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 5] = 5\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 4] = 6\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 3] = 7\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 2] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = itest[itest.columns]\n",
    "z = itrain[itrain.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to write a function that compares the itest and itrain\n",
    "# and checks for columns present in the itrain but absent in the itest\n",
    "for i in z:\n",
    "    if i in y:\n",
    "        pass\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.drop(['Year_of_recruitment'], axis=1, inplace=True)\n",
    "itest.drop(['Year_of_recruitment'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.drop(['Year_of_birth_1950', 'Year_of_birth_1952', 'Year_of_birth_1955', 'Year_of_birth_1956', 'Year_of_birth_1957'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 'More than 5'] = 0\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 5] = 1\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 4] = 2\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 3] = 3\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 2] = 4\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 1] = 5\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 0] = 6\n",
    "\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 'More than 5'] = 0\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 5] = 1\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 4] = 2\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 3] = 3\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 2] = 4\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 1] = 5\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 0] = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Some of these columns are not integers. Some are objects, category, etc.\n",
    "Lets convert it all to numeric form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Qualification = pd.to_numeric(itrain.Qualification)\n",
    "itrain.Foreign_schooled = pd.to_numeric(itrain.Foreign_schooled)\n",
    "itrain.Training_score_average = pd.to_numeric(itrain.Training_score_average)\n",
    "itrain.Past_Disciplinary_Action = pd.to_numeric(itrain.Past_Disciplinary_Action)\n",
    "itrain.No_of_previous_employers = pd.to_numeric(itrain.No_of_previous_employers)\n",
    "itrain.Last_performance_score = pd.to_numeric(itrain.Last_performance_score)\n",
    "\n",
    "itest.Qualification = pd.to_numeric(itest.Qualification)\n",
    "itest.Foreign_schooled = pd.to_numeric(itest.Foreign_schooled)\n",
    "itest.Past_Disciplinary_Action = pd.to_numeric(itest.Past_Disciplinary_Action)\n",
    "itest.No_of_previous_employers = pd.to_numeric(itest.No_of_previous_employers)\n",
    "itest.Training_score_average = pd.to_numeric(itest.Training_score_average)\n",
    "itest.Last_performance_score = pd.to_numeric(itest.Last_performance_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.shape, itest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Some of these columns contain null values. \n",
    "Lets fill them up with their mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in itrain.columns:\n",
    "    itrain[i].fillna(itrain[i].mode()[0], inplace = True)\n",
    "    \n",
    "for i in itest.columns:\n",
    "    itest[i].fillna(itest[i].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Lets check to see if thats all done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itest.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets assign a variable to EmployeeNo and drop it\n",
    "target_Id = itest.EmployeeNo\n",
    "itrain.drop(['EmployeeNo'], axis=1, inplace=True)\n",
    "itest.drop(['EmployeeNo'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a variable for the column we are to predict and drop it\n",
    "target = itrain.Promoted_or_Not\n",
    "itrain.drop(['Promoted_or_Not'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itest.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Since thats all done, Lets move to the next stage!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Making Predictions Using Machine Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the data is highly imbalanced, we can upsample the column\n",
    "# Creating a function \"using_smote\"\n",
    "def using_smote(X, y):\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    sm = SMOTE()\n",
    "    X, y = sm.fit_sample(X, y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the fuction on the itrain and target \n",
    "train_val, target_val = using_smote(itrain, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train,y_test= train_test_split(itrain, target , test_size = 0.22, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.shape, itest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the machine models to be used\n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# import the tools for metrics\n",
    "from sklearn.model_selection import train_test_split #training and testing data split\n",
    "from sklearn import metrics #accuracy measure\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>* Using Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=270, random_state=3, max_features='auto', n_jobs=4, bootstrap=True, class_weight=None, criterion='gini')\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_train, y_train),model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using the X_test\n",
    "pred1 = model.predict(x_test)\n",
    "# Get the f1 score\n",
    "print('The f1_score is', f1_score(pred1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the overall performance of the data\n",
    "from sklearn.metrics import classification_report\n",
    "print('çlassification_report')\n",
    "print(classification_report(pred1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict for the itest data \n",
    "pred1a = model.predict(itest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data = {'EmployeeNo':target_Id, 'Promoted_or_Not':pred1a})\n",
    "output.to_csv(path_or_buf = 'rand-6bumiepredi.csv', index = False, quoting = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Using the Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(learning_rate = 0.2, n_estimators = 270, random_state=5, max_features='auto', max_depth=4)\n",
    "model.fit(a_train, y_train) \n",
    "print('Accuracy score for x_train is ', model.score(x_train, y_train))\n",
    "print('Accuracy score for x_test is ', model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using the x_test\n",
    "pred2 = model.predict(x_test)\n",
    "# predict using the itest data\n",
    "pred2a = model.predict(itest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the f1 score\n",
    "print('The f1_score is',f1_score(pred2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the overall performance of the data\n",
    "from sklearn.metrics import classification_report\n",
    "print('çlassification_report')\n",
    "print(classification_report(pred2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Export It To A Csv File\n",
    "output = pd.DataFrame(data = {'EmployeeNo':target_Id, 'Promoted_or_Not':pred2a})\n",
    "output.to_csv(path_or_buf = 'gb-4bumiepredi.csv', index = False, quoting = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>* Using XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(learning_rate = 0.2, num_boosting_rounds = 270 , max_depth=10, subsample=0.8)\n",
    "xgb.fit(x_train, y_train)\n",
    "print('The Accuracy Score For x_test Is : ', xgb.score(x_train, y_train)), print('The Accuracy Score For x_test Is : ', xgb.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Lets convert the itest data to an array first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itest = np.array(itest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3 = xgb.predict(x_test)\n",
    "pred3a = xgb.predict(itest)\n",
    "f1_score(pred3, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the overall performance of the data\n",
    "from sklearn.metrics import classification_report\n",
    "print('çlassification_report')\n",
    "print(classification_report(pred3, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data = {'EmployeeNo':target_Id, 'Promoted_or_Not':pred})\n",
    "output.to_csv(path_or_buf = 'xgb-1bumiepredi.csv', index = False, quoting = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Lets try stacking models together to see if we would have a better precision and recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vecstack import stacking\n",
    "from xgboost import XGBClassifier\n",
    "models = [RandomForestClassifier(n_estimators=270, random_state=3, max_features='auto', n_jobs=2, bootstrap=True, class_weight=None, criterion='gini'), GradientBoostingClassifier(learning_rate = 0.2, n_estimators = 270, random_state=5, max_features='auto', max_depth=5), XGBClassifier(learning_rate = 0.2, num_boosting_rounds = 270 , max_depth=3, subsample=0.8)]\n",
    "a_train, a_test = stacking(models,                   \n",
    "                           x_train, y_train, x_test,   \n",
    "                           regression=False, \n",
    "     \n",
    "                           mode='oof_pred_bag', \n",
    "       \n",
    "                           needs_proba=False,\n",
    "         \n",
    "                           save_dir=None, \n",
    "            \n",
    "                           metric=f1_score, \n",
    "    \n",
    "                           n_folds=5, \n",
    "                 \n",
    "                           stratified=True,\n",
    "            \n",
    "                           shuffle=True,  \n",
    "            \n",
    "                           random_state=0,    \n",
    "         \n",
    "                           verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Lets save the predictions from random forest, gradient boosting and xgboost classification as a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test =  pd.DataFrame(pred1a)\n",
    "array = pred2a\n",
    "array1 = pred3a\n",
    "new_test[1] = array\n",
    "new_test[2] = array1\n",
    "new_test = np.array(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_train['1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Lets now test the new test and the new train(a_train, a_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(a_train, y_train)\n",
    "lr.score(a_train, y_train), lr.score(a_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets test it on the x_test data\n",
    "pred = lr.predict(a_test)\n",
    "# predict using the itest data\n",
    "pred3a = lr.predict(b_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the f1 score\n",
    "f1_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the overall performance of the data\n",
    "from sklearn.metrics import classification_report\n",
    "print('çlassification_report')\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data = {'EmployeeNo':target_Id, 'Promoted_or_Not':pred3a})\n",
    "output.to_csv(path_or_buf = 'lr-1bumiepredi.csv', index = False, quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm #support vector Machine\n",
    "model=svm.SVC(kernel='',C=1,gamma=0.1) \n",
    "model.fit(a_train, y_train)\n",
    "model.score(a_train, y_train), model.score(a_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets test it on the x_test data\n",
    "pred = model.predict(a_test)\n",
    "# predict using the itest data\n",
    "pred3a = model.predict(b_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the f1 score\n",
    "f1_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the overall performance of the data\n",
    "from sklearn.metrics import classification_report\n",
    "print('çlassification_report')\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data = {'EmployeeNo':target_Id, 'Promoted_or_Not':pred3a})\n",
    "output.to_csv(path_or_buf = 'svc-1bumiepredi.csv', index = False, quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
