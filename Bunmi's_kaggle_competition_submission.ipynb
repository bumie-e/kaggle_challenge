{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Name : Akinremi Bunmi Josephine.\n",
    "\n",
    "<h3>kaggle_Id : bumie.\n",
    "\n",
    "<h3>Student_Status: 100 level.\n",
    "\n",
    "<h3>Gender : Female.\n",
    "\n",
    "<h3>Special_Services : AI+ Club Campus Co-ordinator\n",
    "\n",
    "<h3>Name_of_School : Obafemi Awolowo University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Important\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import the testdata, train data, submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "sam = pd.read_csv('sample_submission2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lets take a brief look at the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> We can see that the train dataset has 38312 rows and 19 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Lets take a brief look at the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> We can see that the test dataset has 16496 columns and 18 rows</b>\n",
    "\n",
    "<h3> Does our dataset contain any null value? Lets find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Hmmn, we can see that the column \"Qualification\" has only 36633 of its values that are not null. \n",
    "<h3> We can also see the various datatypes present in the dataset.(We'll deal with these later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lets divide the dataset into biased and unbiased. \n",
    "    \n",
    "\n",
    "<h3>Lets first take a look at the unbiased dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>The Unbiased dataset contains the following columns: Qualification, Target_met, Previous_Award, Trainning_Score_Average,\n",
    "<h4>Foregin_Schooled, Past_Disiciplanry_Action, Previous_Intradepartmental_Movement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> First of all, lets create a dataframe containing the test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain = train\n",
    "itest = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lets Review the Qualification Column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Qualification.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>For better view let's plot these columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(x, y):\n",
    "    fig, ax = plt.subplots(figsize = (13, 4))\n",
    "    sns.barplot(x, y)\n",
    "    plt.xlabel('x', fontSize=13)\n",
    "    plt.xticks(rotation='90')\n",
    "    plt.ylabel('Promoted_or_Not', fontSize=13)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test.Qualification\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
    "train['Qualification'].value_counts().plot.pie(explode=[0,0.1,0.2],autopct='%1.1f%%',ax=ax[0], shadow=True)\n",
    "ax[0].set_title('Qualification vs promotion')\n",
    "ax[0].set_ylabel('Qualification')\n",
    "sns.factorplot('Qualification', 'Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Qualification vs promotion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h4> From the above, we can say that more of the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Review the Target_met column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Targets_met.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Targets_met','Promoted_or_Not',hue='Targets_met',data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
    "train[['Targets_met','Promoted_or_Not']].groupby(['Targets_met']).mean().plot.bar(ax=ax[0])\n",
    "ax[0].set_title('Targets_met vs Promoted_or_Not')\n",
    "sns.countplot('Targets_met',hue='Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Targets_met:Promoted or Not')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test.Targets_met\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets review the Previous_Award column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Previous_Award.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Review the Promoted_or_Not Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better view let's plot these percents!\n",
    "x = test.Previous_Award\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "sns.barplot('Previous_Award','Promoted_or_Not',data=train,ax=ax[0])\n",
    "ax[0].set_title('Previous_Award vs Promoted_or_Not')\n",
    "sns.factorplot('Previous_Award','Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Previous_Award vs Promoted_or_Not')\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Previous_Award','Promoted_or_Not',hue='Previous_Award',data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets review the Trainning score average column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train.Training_score_average\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Training_score_average','Promoted_or_Not',data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets review the Foregin_Schooled column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Foreign_schooled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test.Foreign_schooled\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "sns.distplot(train[train['Foreign_schooled']=='Yes'].Promoted_or_Not,ax=ax[0])\n",
    "ax[0].set_title('Schooled Aborad:Yes')\n",
    "sns.distplot(train[train['Foreign_schooled']=='No'].Promoted_or_Not,ax=ax[1])\n",
    "ax[1].set_title('Schooled Abroad:No')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the Past_Disciplinary_Action column!,\n",
    "\n",
    "And see those who has been disciplined(bad boys) before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Past_Disciplinary_Action.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Past_Disciplinary_Action','Promoted_or_Not',data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test.Past_Disciplinary_Action\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "sns.distplot(train[train['Past_Disciplinary_Action']=='Yes'].Promoted_or_Not,ax=ax[0])\n",
    "ax[0].set_title('Past_Disciplinary_Action:Yes')\n",
    "sns.distplot(train[train['Past_Disciplinary_Action']=='No'].Promoted_or_Not,ax=ax[1])\n",
    "ax[1].set_title('Past_Disciplinary_Action:No')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the Previous_Intradepartmental_Movement column. Some might have even been promoted before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Previous_IntraDepartmental_Movement','Promoted_or_Not',data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = itrain.Previous_IntraDepartmental_Movement\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
    "train[['Previous_IntraDepartmental_Movement','Promoted_or_Not']].groupby(['Previous_IntraDepartmental_Movement']).mean().plot.bar(ax=ax[0])\n",
    "ax[0].set_title('Previous_IntraDepartmental_Movement vs Promoted_or_Not')\n",
    "sns.countplot('Previous_IntraDepartmental_Movement',hue='Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Previous_IntraDepartmental_Movement:Promoted or Not')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,15))\n",
    "sns.countplot('Previous_IntraDepartmental_Movement',data=train,ax=ax[0])\n",
    "ax[0].set_title('Previous_IntraDepartmental_Movement')\n",
    "sns.countplot('Previous_IntraDepartmental_Movement',hue='Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Previous_IntraDepartmental_Movement for Promoted_or_Not')\n",
    "plt.subplots_adjust(wspace=0.2,hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after we are done with that, lets take a look at the Biased dataset.\n",
    "\n",
    "<h4>The Biased dataset contains the following : </h4>\n",
    "Division, Marital_Status, Gender, Channel_of_Recruitment, Year_of_birth, Trainings_Attended, Last_performance_score, No_of_previous employers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train.Division\n",
    "y = train.Promoted_or_Not\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test.Division\n",
    "y = train.Promoted_or_Not\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Division', 'Promoted_or_Not',data=train)\n",
    "plt.xlabel('Division Columns', fontSize=13)\n",
    "plt.xticks(rotation='90')\n",
    "plt.ylabel('Promoted_or_Not', fontSize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(y,x,orient = 'h')\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
    "train['Division'].value_counts().plot.pie(explode=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8],autopct='%1.1f%%',ax=ax[0], shadow=True)\n",
    "ax[0].set_title('Division vs promotion')\n",
    "ax[0].set_ylabel('Division')\n",
    "sns.factorplot('Division', 'Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Division vs promotion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> From these, we can see that this column is biased as some divisions are more promoted than some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test.Gender\n",
    "y = train.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better view let's plot these percents!\n",
    "x = train.Gender\n",
    "y = train.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "train['Gender'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0], shadow=True)\n",
    "ax[0].set_title('Gender vs Promoted_or_Not')\n",
    "sns.factorplot('Gender','Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Gender vs Promoted_or_Not')\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> From here, we can see that there are more males than females but, more females are promoted than males!\n",
    "    \n",
    "<h3> Lets look at the Channel_of_Recruitment column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Channel_of_Recruitment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test.Channel_of_Recruitment\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better view let's plot these percents!\n",
    "x = train.Channel_of_Recruitment\n",
    "y = train.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "sns.barplot('Channel_of_Recruitment','Promoted_or_Not',data=train,ax=ax[0])\n",
    "ax[0].set_title('Channel_of_Recruitment vs Promoted_or_Not')\n",
    "sns.factorplot('Channel_of_Recruitment','Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Channel_of_Recruitment vs Promoted_or_Not')\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Am sure you'll agree that with me that those who had special referral even got promoted than those who cme through the normal qualified way\n",
    "    \n",
    "<h3> So now, Lets take a look at the Trainings_Attended column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Trainings_Attended.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train.Trainings_Attended\n",
    "y = train.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(y,x,orient = 'h')\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "sns.barplot('Trainings_Attended','Promoted_or_Not',data=train,ax=ax[0])\n",
    "ax[0].set_title('Trainings_Attended vs Promoted_or_Not')\n",
    "sns.factorplot('Trainings_Attended','Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Trainings_Attended vs Promoted_or_Not')\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Well, its just obivious that these employees were not promoted based on their performance\n",
    "\n",
    "<h3> Without much ado, lets move to the Year_of_birth column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Year_of_birth.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better view let's plot these percents!\n",
    "x = train.Year_of_birth\n",
    "y = train.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "sns.barplot('Year_of_birth','Promoted_or_Not',data=train,ax=ax[0])\n",
    "ax[0].set_title('Year_of_birth vs Promoted_or_Not')\n",
    "sns.factorplot('Year_of_birth','Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Year_of_birth vs Promoted_or_Not')\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.barplot(y,x,orient = 'h')\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> From these, we can really tell whose who because they are ll closely packed together\n",
    "\n",
    "<h3> Now, moving on to Last_performance_score column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Last_performance_score.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test.Last_performance_score\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better view let's plot these percents!\n",
    "x = itrain.Last_performance_score\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(y,x,orient = 'h')\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'NaN' in itrain.Last_performance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "sns.barplot('Last_performance_score','Promoted_or_Not',data=train,ax=ax[0])\n",
    "ax[0].set_title('Last_performance_score vs Promoted_or_Not')\n",
    "sns.factorplot('Last_performance_score','Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Last_performance_score vs Promoted_or_Not')\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> We can also see some imbalances especially with those who scored 0.0 points\n",
    "\n",
    "<h3>Ok, lets look at the Year_of_recruitment column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Year_of_recruitment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test.Year_of_recruitment\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = itrain.Year_of_recruitment\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Year_of_recruitment','Promoted_or_Not',hue='Promoted_or_Not',data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
    "train[['Year_of_recruitment','Promoted_or_Not']].groupby(['Year_of_recruitment']).mean().plot.bar(ax=ax[0])\n",
    "ax[0].set_title('Promoted_or_Not vs Year_of_recruitment')\n",
    "sns.countplot('Year_of_recruitment',hue='Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Year_of_recruitment:Promoted or Not')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "sns.barplot('Year_of_recruitment','Promoted_or_Not',data=train,ax=ax[0])\n",
    "ax[0].set_title('Year_of_recruitment vs Promoted_or_Not')\n",
    "sns.factorplot('Year_of_recruitment','Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Year_of_recruitment vs Promoted_or_Not')\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> We can also see imbalances here too with this column\n",
    "    \n",
    "<h3> Next! Taking a look at the State_Of_Origin column,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.State_Of_Origin.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better view let's plot these percents!\n",
    "x = itrain.State_Of_Origin\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test.State_Of_Origin\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('State_Of_Origin','Promoted_or_Not',hue='State_Of_Origin',data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "sns.barplot('State_Of_Origin','Promoted_or_Not',data=train,ax=ax[0])\n",
    "ax[0].set_title('State_Of_Origin vs Promoted_or_Not')\n",
    "sns.factorplot('State_Of_Origin','Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('State_Of_Origin vs Promoted_or_Not')\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Also in this column, we can see various irregularities in the pattern\n",
    "\n",
    "\n",
    "<h3> Now, lets look at the Foreign_schooled column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Marital_Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "sns.barplot('Marital_Status','Promoted_or_Not',data=train,ax=ax[0])\n",
    "ax[0].set_title('Marital_Status vs Promoted_or_Not')\n",
    "sns.factorplot('Marital_Status','Promoted_or_Not',data=train,ax=ax[1])\n",
    "ax[1].set_title('Marital_Status vs Promoted_or_Not')\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test.Marital_Status\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Marital_Status','Promoted_or_Not',hue='Marital_Status',data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train.Marital_Status\n",
    "y = train.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.No_of_previous_employers.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better view let's plot these percents!\n",
    "x = itrain.No_of_previous_employers\n",
    "y = itrain.Promoted_or_Not\n",
    "\n",
    "plot_graph(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('No_of_previous_employers','Promoted_or_Not',hue='No_of_previous_employers',data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
    "itrain['No_of_previous_employers'].value_counts().plot.pie(explode=[0,0.1,0.2,0.3,0.4,0.5,0.6],autopct='%1.1f%%',ax=ax[0], shadow=True)\n",
    "ax[0].set_title('No_of_previous_employers vs promotion')\n",
    "ax[0].set_ylabel('No_of_previous_employers')\n",
    "sns.factorplot('No_of_previous_employers', 'Promoted_or_Not',data=itrain,ax=ax[1])\n",
    "ax[1].set_title('No_of_previous_employers vs promotion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Here also, we can basically see the irregularity in thr promoting pattern\n",
    "    \n",
    "<h3> So now, we are basically done with visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Now, lets Label Encode and One Hot Encode our data, where neccesary and drop few unimportant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = np.array(train.Last_performance_score)\n",
    "i2 = np.array(train.Trainings_Attended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can not merge DataFrame with instance of type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-4c14967b3413>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     59\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                          validate=validate)\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    522\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             raise ValueError('can not merge DataFrame with instance of '\n\u001b[1;32m--> 524\u001b[1;33m                              'type {left}'.format(left=type(left)))\n\u001b[0m\u001b[0;32m    525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m             raise ValueError('can not merge DataFrame with instance of '\n",
      "\u001b[1;31mValueError\u001b[0m: can not merge DataFrame with instance of type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "mm = pd.merge(i1, i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain = pd.get_dummies(train, columns = ['Marital_Status', 'Division', 'Previous_IntraDepartmental_Movement', 'State_Of_Origin', 'Year_of_birth','No_of_previous_employers'])\n",
    "itest = pd.get_dummies(test, columns = ['Marital_Status', 'Division', 'Previous_IntraDepartmental_Movement', 'State_Of_Origin', 'Year_of_birth', 'No_of_previous_employers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain['Qualification'][itrain['Qualification'] == 'MSc, MBA and PhD'] = 2\n",
    "itrain['Qualification'][itrain['Qualification'] == 'First Degree or HND'] = 0\n",
    "itrain['Qualification'][itrain['Qualification'] == 'Non-University Education'] = 1\n",
    "\n",
    "\n",
    "itest['Qualification'][itest['Qualification'] == 'MSc, MBA and PhD'] = 2\n",
    "itest['Qualification'][itest['Qualification'] == 'First Degree or HND'] = 0\n",
    "itest['Qualification'][itest['Qualification'] == 'Non-University Education'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain['Targets_met'][itrain['Targets_met'] == 1] = 0\n",
    "itrain['Targets_met'][itrain['Targets_met'] == 0] = 1\n",
    "\n",
    "itest['Targets_met'][itest['Targets_met'] == 1] = 0\n",
    "itest['Targets_met'][itest['Targets_met'] == 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain['Previous_Award'][itrain['Previous_Award'] == 1] = 0\n",
    "itrain['Previous_Award'][itrain['Previous_Award'] == 0] = 1\n",
    "\n",
    "itest['Previous_Award'][itest['Previous_Award'] == 1] = 0\n",
    "itest['Previous_Award'][itest['Previous_Award'] == 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Foreign_schooled[itrain.Foreign_schooled == 'Yes'] = 0\n",
    "itrain.Foreign_schooled[itrain.Foreign_schooled == 'No'] = 1\n",
    "\n",
    "itest.Foreign_schooled[itest.Foreign_schooled == 'Yes'] = 0\n",
    "itest.Foreign_schooled[itest.Foreign_schooled == 'No'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Past_Disciplinary_Action[itrain.Past_Disciplinary_Action == 'No'] = 0\n",
    "itrain.Past_Disciplinary_Action[itrain.Past_Disciplinary_Action == 'Yes'] = 1\n",
    "\n",
    "itest.Past_Disciplinary_Action[itest.Past_Disciplinary_Action == 'No'] = 0\n",
    "itest.Past_Disciplinary_Action[itest.Past_Disciplinary_Action == 'Yes'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 12.5] = 0\n",
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 10.0] = 1\n",
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 7.5] = 2\n",
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 5.0] = 3\n",
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 2.5] = 4\n",
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 0.0] = 5\n",
    "\n",
    "\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 12.5] = 0\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 10.0] = 1\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 7.5] = 2\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 5.0] = 3\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 2.5] = 4\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 0.0] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 11] = 0\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 10] = 1\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 9] = 2\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 8] = 3\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 7] = 4\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 6] = 5\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 5] = 6\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 4] = 7\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 3] = 8\n",
    "itrain['Trainings_Attended'][itrain['Trainings_Attended'] == 2] = 9\n",
    "\n",
    "\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 11] = 0\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 9] = 1\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 8] = 2\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 7] = 3\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 6] = 4\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 5] = 5\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 4] = 6\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 3] = 7\n",
    "itest['Trainings_Attended'][itest['Trainings_Attended'] == 2] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = itest[itest.columns]\n",
    "z = itrain[itrain.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promoted_or_Not\n",
      "Year_of_birth_1950\n",
      "Year_of_birth_1952\n",
      "Year_of_birth_1955\n",
      "Year_of_birth_1956\n",
      "Year_of_birth_1957\n"
     ]
    }
   ],
   "source": [
    "# to write a function that compares the itest and itrain\n",
    "# and checks for columns present in the itrain but absent in the itest\n",
    "for i in z:\n",
    "    if i in y:\n",
    "        pass\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.drop(['Year_of_recruitment','Channel_of_Recruitment', 'Gender'], axis=1, inplace=True)\n",
    "itest.drop(['Year_of_recruitment','Channel_of_Recruitment', 'Gender'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.drop(['Year_of_birth_1950', 'Year_of_birth_1952', 'Year_of_birth_1955', 'Year_of_birth_1956', 'Year_of_birth_1957'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 'More than 5'] = 0\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 5] = 1\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 4] = 2\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 3] = 3\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 2] = 4\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 1] = 5\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 0] = 6\n",
    "\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 'More than 5'] = 0\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 5] = 1\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 4] = 2\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 3] = 3\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 2] = 4\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 1] = 5\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 0] = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Some of these columns are not integers. Some are objects, category, etc.\n",
    "Lets convert it all to numeric form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Qualification = pd.to_numeric(itrain.Qualification)\n",
    "itrain.Foreign_schooled = pd.to_numeric(itrain.Foreign_schooled)\n",
    "itrain.Training_score_average = pd.to_numeric(itrain.Training_score_average)\n",
    "itrain.Past_Disciplinary_Action = pd.to_numeric(itrain.Past_Disciplinary_Action)\n",
    "itrain.No_of_previous_employers = pd.to_numeric(itrain.No_of_previous_employers)\n",
    "itrain.Last_performance_score = pd.to_numeric(itrain.Last_performance_score)\n",
    "\n",
    "itest.Qualification = pd.to_numeric(itest.Qualification)\n",
    "itest.Foreign_schooled = pd.to_numeric(itest.Foreign_schooled)\n",
    "itest.Past_Disciplinary_Action = pd.to_numeric(itest.Past_Disciplinary_Action)\n",
    "itest.No_of_previous_employers = pd.to_numeric(itest.No_of_previous_employers)\n",
    "itest.Training_score_average = pd.to_numeric(itest.Training_score_average)\n",
    "itest.Last_performance_score = pd.to_numeric(itest.Last_performance_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38312, 106), (16496, 105))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itrain.shape, itest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Some of these columns contain null values. \n",
    "Lets fill them up with their mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in itrain.columns:\n",
    "    itrain[i].fillna(itrain[i].mode()[0], inplace = True)\n",
    "    \n",
    "for i in itest.columns:\n",
    "    itest[i].fillna(itest[i].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Lets check to see if thats all done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38312 entries, 0 to 38311\n",
      "Columns: 106 entries, EmployeeNo to Year_of_birth_2001\n",
      "dtypes: float64(2), int64(8), object(1), uint8(95)\n",
      "memory usage: 6.7+ MB\n"
     ]
    }
   ],
   "source": [
    "itrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itest.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets assign a variable to EmployeeNo and drop it\n",
    "target_Id = itest.EmployeeNo\n",
    "itrain.drop(['EmployeeNo'], axis=1, inplace=True)\n",
    "itest.drop(['EmployeeNo'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a variable for the column we are to predict and drop it\n",
    "target = itrain.Promoted_or_Not\n",
    "itrain.drop(['Promoted_or_Not'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itest.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Since thats all done, Lets move to the next stage!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Making Predictions Using Machine Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the data is highly imbalanced, we can upsample the column\n",
    "# Creating a function \"using_smote\"\n",
    "def using_smote(X, y):\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    sm = SMOTE()\n",
    "    X, y = sm.fit_sample(X, y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the fuction on the itrain and target \n",
    "train_val, target_val = using_smote(itrain, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train,y_test= train_test_split(itrain, target , test_size = 0.22, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38312, 104), (16496, 104))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itrain.shape, itest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the machine models to be used\n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# import the tools for metrics\n",
    "from sklearn.model_selection import train_test_split #training and testing data split\n",
    "from sklearn import metrics #accuracy measure\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>* Using Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9986279824649467, 0.933918614307747)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=270, random_state=3, max_features='auto', n_jobs=4, bootstrap=True, class_weight=None, criterion='gini')\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_train, y_train),model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1_score is 0.41058201058201055\n"
     ]
    }
   ],
   "source": [
    "# predict using the X_test\n",
    "pred1 = model.predict(x_test)\n",
    "# Get the f1 score\n",
    "print('The f1_score is', f1_score(pred1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "çlassification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.96      8204\n",
      "           1       0.27      0.86      0.41       225\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      8429\n",
      "   macro avg       0.63      0.90      0.69      8429\n",
      "weighted avg       0.98      0.93      0.95      8429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see the overall performance of the data\n",
    "from sklearn.metrics import classification_report\n",
    "print('çlassification_report')\n",
    "print(classification_report(pred1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict for the itest data \n",
    "pred1a = model.predict(itest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data = {'EmployeeNo':target_Id, 'Promoted_or_Not':pred1a})\n",
    "output.to_csv(path_or_buf = 'rand-6bumiepredi.csv', index = False, quoting = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Using the Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for x_train is  0.9986279824649467\n",
      "Accuracy score for x_test is  0.9231225530905208\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier(learning_rate = 0.2, n_estimators = 270, random_state=5, max_features='auto', max_depth=10)\n",
    "model.fit(x_train, y_train) \n",
    "print('Accuracy score for x_train is ', model.score(x_train, y_train))\n",
    "print('Accuracy score for x_test is ', model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using the x_test\n",
    "pred2 = model.predict(x_test)\n",
    "# predict using the itest data\n",
    "pred2a = model.predict(itest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1_score is 0.41935483870967744\n"
     ]
    }
   ],
   "source": [
    "# get the f1 score\n",
    "print('The f1_score is',f1_score(pred2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "çlassification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96      8033\n",
      "           1       0.33      0.59      0.42       396\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      8429\n",
      "   macro avg       0.65      0.77      0.69      8429\n",
      "weighted avg       0.95      0.92      0.93      8429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see the overall performance of the data\n",
    "from sklearn.metrics import classification_report\n",
    "print('çlassification_report')\n",
    "print(classification_report(pred2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Export It To A Csv File\n",
    "output = pd.DataFrame(data = {'EmployeeNo':target_Id, 'Promoted_or_Not':pred2a})\n",
    "output.to_csv(path_or_buf = 'gb-4bumiepredi.csv', index = False, quoting = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>* Using XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy Score For x_test Is :  0.9495030619415721\n",
      "The Accuracy Score For x_test Is :  0.9384268596512042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(learning_rate = 0.2, num_boosting_rounds = 270 , max_depth=10, subsample=0.8)\n",
    "xgb.fit(x_train, y_train)\n",
    "print('The Accuracy Score For x_test Is : ', xgb.score(x_train, y_train)), print('The Accuracy Score For x_test Is : ', xgb.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Qualification', 'Trainings_Attended', 'Last_performance_score',\n",
       "       'Targets_met', 'Previous_Award', 'Training_score_average',\n",
       "       'Foreign_schooled', 'Past_Disciplinary_Action',\n",
       "       'No_of_previous_employers', 'Marital_Status_Married',\n",
       "       ...\n",
       "       'Year_of_birth_1992', 'Year_of_birth_1993', 'Year_of_birth_1994',\n",
       "       'Year_of_birth_1995', 'Year_of_birth_1996', 'Year_of_birth_1997',\n",
       "       'Year_of_birth_1998', 'Year_of_birth_1999', 'Year_of_birth_2000',\n",
       "       'Year_of_birth_2001'],\n",
       "      dtype='object', length=104)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itrain.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Lets convert the itest data to an array first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "itest = np.array(itest)\n",
    "col = [x for x in itrain.columns]\n",
    "\n",
    "itest = pd.DataFrame(itest, columns = col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_prediction = xgb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45881126173096975"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred3 = xgb.predict(x_test)\n",
    "#pred3a = xgb.predict(itest)\n",
    "f1_score(pred3, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "çlassification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97      8190\n",
      "           1       0.31      0.92      0.46       239\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      8429\n",
      "   macro avg       0.65      0.93      0.71      8429\n",
      "weighted avg       0.98      0.94      0.95      8429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see the overall performance of the data\n",
    "from sklearn.metrics import classification_report\n",
    "print('çlassification_report')\n",
    "print(classification_report(pred3, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd1 = pd.read_csv('gbbumiepredi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7690   19]\n",
      " [ 500  220]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data = {'EmployeeNo':target_Id, 'Promoted_or_Not':xgb_prediction})\n",
    "output.to_csv(path_or_buf = 'xgb-3bumiepredi.csv', index = False, quoting = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Lets try stacking models together to see if we would have a better precision and recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [2]\n",
      "metric:       [f1_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [3]\n",
      "\n",
      "model  0:     [RandomForestClassifier]\n",
      "    fold  0:  [0.31699346]\n",
      "    fold  1:  [0.37558685]\n",
      "    fold  2:  [0.33118971]\n",
      "    fold  3:  [0.38425197]\n",
      "    fold  4:  [0.38184664]\n",
      "    ----\n",
      "    MEAN:     [0.35797373] + [0.02816895]\n",
      "    FULL:     [0.35843661]\n",
      "\n",
      "model  1:     [GradientBoostingClassifier]\n",
      "    fold  0:  [0.45418327]\n",
      "    fold  1:  [0.47106326]\n",
      "    fold  2:  [0.48157895]\n",
      "    fold  3:  [0.47346939]\n",
      "    fold  4:  [0.45646438]\n",
      "    ----\n",
      "    MEAN:     [0.46735185] + [0.01044565]\n",
      "    FULL:     [0.46732462]\n",
      "\n",
      "model  2:     [XGBClassifier]\n",
      "    fold  0:  [0.46917293]\n",
      "    fold  1:  [0.47916667]\n",
      "    fold  2:  [0.49404762]\n",
      "    fold  3:  [0.48142645]\n",
      "    fold  4:  [0.49258160]\n",
      "    ----\n",
      "    MEAN:     [0.48327905] + [0.00918505]\n",
      "    FULL:     [0.48331347]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking\n",
    "from xgboost import XGBClassifier\n",
    "models = [RandomForestClassifier(n_estimators=270, random_state=3, max_features='auto', n_jobs=2, bootstrap=True, class_weight=None, criterion='gini'), GradientBoostingClassifier(learning_rate = 0.2, n_estimators = 270, random_state=5, max_features='auto', max_depth=5), XGBClassifier(learning_rate = 0.2, num_boosting_rounds = 270 , max_depth=3, subsample=0.8)]\n",
    "a_train, a_test = stacking(models,                   \n",
    "                           x_train, y_train, x_test,   \n",
    "                           regression=False, \n",
    "     \n",
    "                           mode='oof_pred_bag', \n",
    "       \n",
    "                           needs_proba=False,\n",
    "         \n",
    "                           save_dir=None, \n",
    "            \n",
    "                           metric=f1_score, \n",
    "    \n",
    "                           n_folds=5, \n",
    "                 \n",
    "                           stratified=True,\n",
    "            \n",
    "                           shuffle=True,  \n",
    "            \n",
    "                           random_state=0,    \n",
    "         \n",
    "                           verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Lets save the predictions from random forest, gradient boosting and xgboost classification as a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test =  pd.DataFrame(pred1a)\n",
    "array = pred2a\n",
    "array1 = pred3a\n",
    "new_test[1] = array\n",
    "new_test[2] = array1\n",
    "new_test = np.array(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Lets now test the new test and the new train(a_train, a_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9419736974199377, 0.9384268596512042)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(a_train, y_train)\n",
    "lr.score(a_train, y_train), lr.score(a_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets test it on the x_test data\n",
    "pred = lr.predict(a_test)\n",
    "# predict using the itest data\n",
    "pred3a = lr.predict(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44962884411452814"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the f1 score\n",
    "f1_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "çlassification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97      8206\n",
      "           1       0.29      0.95      0.45       223\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      8429\n",
      "   macro avg       0.65      0.94      0.71      8429\n",
      "weighted avg       0.98      0.94      0.95      8429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see the overall performance of the data\n",
    "from sklearn.metrics import classification_report\n",
    "print('çlassification_report')\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7698   11]\n",
      " [ 508  212]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data = {'EmployeeNo':target_Id, 'Promoted_or_Not':pred3a})\n",
    "output.to_csv(path_or_buf = 'lr-2bumiepredi.csv', index = False, quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-faf257910d84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msvm\u001b[0m \u001b[1;31m#support vector Machine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    269\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\svm\\libsvm.pyx\u001b[0m in \u001b[0;36msklearn.svm.libsvm.fit\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: '' is not in list"
     ]
    }
   ],
   "source": [
    "from sklearn import svm #support vector Machine\n",
    "model=svm.SVC(kernel='',C=1,gamma=0.1) \n",
    "model.fit(a_train, y_train)\n",
    "model.score(a_train, y_train), model.score(a_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets test it on the x_test data\n",
    "pred = model.predict(a_test)\n",
    "# predict using the itest data\n",
    "pred3a = model.predict(b_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9678403288906572"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the f1 score\n",
    "f1_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "çlassification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97      8103\n",
      "           1       0.94      1.00      0.97      7329\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     15432\n",
      "   macro avg       0.97      0.97      0.97     15432\n",
      "weighted avg       0.97      0.97      0.97     15432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see the overall performance of the data\n",
    "from sklearn.metrics import classification_report\n",
    "print('çlassification_report')\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy Score For x_test Is :  0.9419736974199377\n",
      "The Accuracy Score For x_test Is :  0.9384268596512042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(learning_rate = 0.2, num_boosting_rounds = 270 , max_depth=10, subsample=0.8)\n",
    "xgb.fit(a_train, y_train)\n",
    "print('The Accuracy Score For x_test Is : ', xgb.score(a_train, y_train)), print('The Accuracy Score For x_test Is : ', xgb.score(a_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets test it on the x_test data\n",
    "pred = xgb.predict(a_test)\n",
    "# predict using the itest data\n",
    "pred3a = xgb.predict(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "çlassification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97      8206\n",
      "           1       0.29      0.95      0.45       223\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      8429\n",
      "   macro avg       0.65      0.94      0.71      8429\n",
      "weighted avg       0.98      0.94      0.95      8429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see the overall performance of the data\n",
    "from sklearn.metrics import classification_report\n",
    "print('çlassification_report')\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7698   11]\n",
      " [ 508  212]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data = {'EmployeeNo':target_Id, 'Promoted_or_Not':pred3a})\n",
    "output.to_csv(path_or_buf = 'bumiepredi.csv', index = False, quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16496 entries, 0 to 16495\n",
      "Data columns (total 2 columns):\n",
      "EmployeeNo         16496 non-null object\n",
      "Promoted_or_Not    16496 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 257.8+ KB\n"
     ]
    }
   ],
   "source": [
    "x = pd.read_csv('bumiepredi (1).csv')\n",
    "x.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    16496\n",
       "Name: Promoted_or_Not, dtype: int64"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.Promoted_or_Not.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
