{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Important\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the datasets\n",
    "test = pd.read_csv('test.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "sam = pd.read_csv('sample_submission2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign values to itrain and itest\n",
    "itrain = train\n",
    "itest = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''itrain.Channel_of_Recruitment[itrain.Channel_of_Recruitment == 'Referral and Special candidates'] = 0\n",
    "itrain.Channel_of_Recruitment[itrain.Channel_of_Recruitment == 'Agency and others'] = 1\n",
    "itrain.Channel_of_Recruitment[itrain.Channel_of_Recruitment == 'Direct Internal process'] = 2\n",
    "\n",
    "itest.Channel_of_Recruitment[itest.Channel_of_Recruitment == 'Referral and Special candidates'] = 0\n",
    "itest.Channel_of_Recruitment[itest.Channel_of_Recruitment == 'Agency and others'] = 1\n",
    "itest.Channel_of_Recruitment[itest.Channel_of_Recruitment == 'Direct Internal process'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns in itrain\n",
    "itrain.drop(['Year_of_birth','State_Of_Origin', 'Trainings_Attended', 'Channel_of_Recruitment', 'Marital_Status'], axis=1, inplace=True)\n",
    "\n",
    "# Drop irrelevant columns in itest\n",
    "itest.drop(['Year_of_birth','State_Of_Origin', 'Trainings_Attended', 'Channel_of_Recruitment', 'Marital_Status'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain = pd.get_dummies(itrain, columns = ['Gender', 'Division'])\n",
    "itest = pd.get_dummies(itest, columns = ['Gender', 'Division'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''itrain.Marital_Status[itrain.Marital_Status == 'Married'] = 1\n",
    "itrain.Marital_Status[itrain.Marital_Status == 'Single'] = 2\n",
    "itrain.Marital_Status[itrain.Marital_Status == 'Not_Sure'] = 0\n",
    "\n",
    "itest.Marital_Status[itest.Marital_Status == 'Married'] = 1\n",
    "itest.Marital_Status[itest.Marital_Status == 'Single'] = 2\n",
    "itest.Marital_Status[itest.Marital_Status == 'Not_Sure'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Previous_IntraDepartmental_Movement[itrain.Previous_IntraDepartmental_Movement == 'No'] = 0\n",
    "itrain.Previous_IntraDepartmental_Movement[itrain.Previous_IntraDepartmental_Movement == 'Yes'] = 1\n",
    "\n",
    "itest.Previous_IntraDepartmental_Movement[itest.Previous_IntraDepartmental_Movement == 'No'] = 0\n",
    "itest.Previous_IntraDepartmental_Movement[itest.Previous_IntraDepartmental_Movement == 'Yes'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#itrain = pd.get_dummies(itrain, columns=['Year_of_recruitment'])\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1982] = 0\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1985] = 1\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1986] = 2\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1987] = 3\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1988] = 4\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1989] = 5\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1990] = 6\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1991] = 7\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1992] = 8\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1993] = 9\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1994] = 10\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1995] = 11\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1996] = 12\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1997] = 13\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1998] = 14\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 1999] = 15\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2000] = 16\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2001] = 17\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2002] = 18\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2003] = 19\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2004] = 20\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2005] = 21\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2006] = 22\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2007] = 23\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2008] = 24\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2009] = 25\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2010] = 26\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2011] = 27\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2012] = 28\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2013] = 29\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2014] = 30\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2015] = 31\n",
    "itrain.Year_of_recruitment[itrain.Year_of_recruitment == 2016] = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#itrain = pd.get_dummies(itrain, columns=['Year_of_recruitment'])\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1982] = 0\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1985] = 1\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1986] = 2\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1987] = 3\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1988] = 4\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1989] = 5\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1990] = 6\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1991] = 7\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1992] = 8\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1993] = 9\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1994] = 10\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1995] = 11\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1996] = 12\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1997] = 13\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1998] = 14\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 1999] = 15\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2000] = 16\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2001] = 17\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2002] = 18\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2003] = 19\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2004] = 20\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2005] = 21\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2006] = 22\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2007] = 23\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2008] = 24\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2009] = 25\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2010] = 26\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2011] = 27\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2012] = 28\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2013] = 29\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2014] = 30\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2015] = 31\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2016] = 32\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2017] = 33\n",
    "itest.Year_of_recruitment[itest.Year_of_recruitment == 2018] = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the rakings for Qualifications\n",
    "itrain['Qualification'][itrain['Qualification'] == 'MSc, MBA and PhD'] = 0\n",
    "itrain['Qualification'][itrain['Qualification'] == 'First Degree or HND'] = 1\n",
    "itrain['Qualification'][itrain['Qualification'] == 'Non-University Education'] = 2\n",
    "\n",
    "# Set the rakings for Qualifications\n",
    "itest['Qualification'][itest['Qualification'] == 'MSc, MBA and PhD'] = 0\n",
    "itest['Qualification'][itest['Qualification'] == 'First Degree or HND'] = 1\n",
    "itest['Qualification'][itest['Qualification'] == 'Non-University Education'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the rakings for the number of previous work experience from employers\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 'More than 5'] = 0\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 5] = 1\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 4] = 2\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 3] = 3\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 2] = 4\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 1] = 5\n",
    "itrain['No_of_previous_employers'][itrain['No_of_previous_employers'] == 0] = 6\n",
    "\n",
    "# Set the rakings for the number of previous work experience from employers\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 'More than 5'] = 0\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 5] = 1\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 4] = 2\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 3] = 3\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 2] = 4\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 1] = 5\n",
    "itest['No_of_previous_employers'][itest['No_of_previous_employers'] == 0] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Targets_met.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the rakings for employees who has met the target for a particular project\n",
    "itrain['Targets_met'][itrain['Targets_met'] == 1] = 0\n",
    "itrain['Targets_met'][itrain['Targets_met'] == 0] = 1\n",
    "\n",
    "# Set the rakings for employees who has met the target for a particular project\n",
    "itest['Targets_met'][itest['Targets_met'] == 1] = 0\n",
    "itest['Targets_met'][itest['Targets_met'] == 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Foreign_schooled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the rakings for employees who schooled abroad and those who didnt\n",
    "itrain.Foreign_schooled[itrain.Foreign_schooled == 'Yes'] = 0\n",
    "itrain.Foreign_schooled[itrain.Foreign_schooled == 'No'] = 1\n",
    "\n",
    "# Set the rakings for employees who schooled abroad and those who didnt\n",
    "itest.Foreign_schooled[itest.Foreign_schooled == 'Yes'] = 0\n",
    "itest.Foreign_schooled[itest.Foreign_schooled == 'No'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.Past_Disciplinary_Action.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the rakings for employees who were previously disciplined and those who were not\n",
    "itrain.Past_Disciplinary_Action[itrain.Past_Disciplinary_Action == 'No'] = 0\n",
    "itrain.Past_Disciplinary_Action[itrain.Past_Disciplinary_Action == 'Yes'] = 1\n",
    "\n",
    "# Set the rakings for employees who were previously disciplined and those who were not\n",
    "itest.Past_Disciplinary_Action[itest.Past_Disciplinary_Action == 'No'] = 0\n",
    "itest.Past_Disciplinary_Action[itest.Past_Disciplinary_Action == 'Yes'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the rakings for employee with the highest Last_performance_score\n",
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 12.5] = 0\n",
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 10.0] = 1\n",
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 7.5] = 2\n",
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 5.0] = 3\n",
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 2.5] = 4\n",
    "itrain['Last_performance_score'][itrain['Last_performance_score'] == 0.0] = 5\n",
    "\n",
    "# Set the rakings for employee with the highest Last_performance_score\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 12.5] = 0\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 10.0] = 1\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 7.5] = 2\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 5.0] = 3\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 2.5] = 4\n",
    "itest['Last_performance_score'][itest['Last_performance_score'] == 0.0] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the datatype to numeric\n",
    "itrain.Qualification = pd.to_numeric(itrain.Qualification)\n",
    "#itrain.Channel_of_Recruitment = pd.to_numeric(itrain.Channel_of_Recruitment)\n",
    "#itrain.Marital_Status = pd.to_numeric(itrain.Marital_Status)\n",
    "itrain.Previous_IntraDepartmental_Movement = pd.to_numeric(itrain.Previous_IntraDepartmental_Movement)\n",
    "itrain.Foreign_schooled = pd.to_numeric(itrain.Foreign_schooled)\n",
    "itrain.Past_Disciplinary_Action = pd.to_numeric(itrain.Past_Disciplinary_Action)\n",
    "itrain.No_of_previous_employers = pd.to_numeric(itrain.No_of_previous_employers)\n",
    "\n",
    "# convert the datatype to numeric\n",
    "itest.Qualification = pd.to_numeric(itest.Qualification)\n",
    "#itest.Channel_of_Recruitment = pd.to_numeric(itest.Channel_of_Recruitment)\n",
    "#itest.Marital_Status = pd.to_numeric(itest.Marital_Status)\n",
    "itest.Previous_IntraDepartmental_Movement = pd.to_numeric(itest.Previous_IntraDepartmental_Movement)\n",
    "itest.Foreign_schooled = pd.to_numeric(itest.Foreign_schooled)\n",
    "itest.Past_Disciplinary_Action = pd.to_numeric(itest.Past_Disciplinary_Action)\n",
    "itest.No_of_previous_employers = pd.to_numeric(itest.No_of_previous_employers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the Submission Id as target_Id, drop it from the given data\n",
    "target_Id = itrain.EmployeeNo\n",
    "itrain.drop(['EmployeeNo'], axis=1, inplace=True)\n",
    "itest.drop(['EmployeeNo'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fillna values with the mode\n",
    "for i in itrain.columns:\n",
    "    itrain[i].fillna(itrain[i].mode()[0], inplace = True)\n",
    "    \n",
    "# Fillna values with the mode\n",
    "for i in itest.columns:\n",
    "    itest[i].fillna(itest[i].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Balancing And Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the given data\n",
    "def using_smote(X, y):\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    sm = SMOTE(ratio = 1.0)\n",
    "    X, y = sm.fit_sample(X, y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the target data and drop it\n",
    "target = itrain.Promoted_or_Not\n",
    "itrain.drop(['Promoted_or_Not'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the values to be balanced\n",
    "x_train, y_train = using_smote(itrain, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train,y_test= train_test_split(x_train, y_train, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52606, 21), (17536, 21))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the machine models to be used\n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# import the tools for metrics\n",
    "from sklearn.model_selection import train_test_split #training and testing data split\n",
    "from sklearn import metrics #accuracy measure\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9825685283047562, 0.9513001824817519)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using Random Forest Classifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_train, y_train), model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using the X_test\n",
    "pred2 = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9505214368482039"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the f1 score\n",
    "f1_score(pred2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict for the itest data \n",
    "predr = model.predict(itest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "çlassification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95      9074\n",
      "           1       0.93      0.97      0.95      8462\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     17536\n",
      "   macro avg       0.95      0.95      0.95     17536\n",
      "weighted avg       0.95      0.95      0.95     17536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see the overall performance of the data\n",
    "from sklearn.metrics import classification_report\n",
    "print('çlassification_report')\n",
    "print(classification_report(pred2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9560126221343573, 0.9572308394160584)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(learning_rate = 0.2, n_estimators = 270)\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_train, y_train), model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using the x_test\n",
    "pred2 = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using the itest data\n",
    "pred = model.predict(itest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9556842354053415"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the f1 score\n",
    "f1_score(pred2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "çlassification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.97      9237\n",
      "           1       0.94      0.99      0.96      8299\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     17536\n",
      "   macro avg       0.96      0.97      0.96     17536\n",
      "weighted avg       0.97      0.96      0.96     17536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see the overall performance of the data\n",
    "from sklearn.metrics import classification_report\n",
    "print('çlassification_report')\n",
    "print(classification_report(pred2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.968634756491655, 0.9642449817518248)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(learning_rate = 0.2, num_boosting_rounds = 270 , max_depth=10, subsample=0.8)\n",
    "xgb.fit(x_train, y_train)\n",
    "xgb.score(x_train, y_train), xgb.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred2 = xgb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.963326899456045"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the f1 score\n",
    "f1_score(pred2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20'] ['Qualification', 'Last_performance_score', 'Year_of_recruitment', 'Targets_met', 'Previous_Award', 'Training_score_average', 'Foreign_schooled', 'Past_Disciplinary_Action', 'Previous_IntraDepartmental_Movement', 'No_of_previous_employers', 'Gender_Female', 'Gender_Male', 'Division_Business Finance Operations', 'Division_Commercial Sales and Marketing', 'Division_Customer Support and Field Operations', 'Division_Information Technology and Solution Support', 'Division_Information and Strategy', 'Division_People/HR Management', 'Division_Regulatory and Legal services', 'Division_Research and Innovation', 'Division_Sourcing and Purchasing']\nexpected f17, f11, f0, f13, f2, f20, f8, f19, f9, f12, f4, f1, f15, f14, f18, f6, f7, f5, f10, f16, f3 in input data\ntraining data did not have the following fields: Past_Disciplinary_Action, Last_performance_score, Gender_Female, Division_Research and Innovation, Division_Information Technology and Solution Support, Division_Commercial Sales and Marketing, Division_Sourcing and Purchasing, Training_score_average, Previous_Award, Division_Information and Strategy, Foreign_schooled, Division_Customer Support and Field Operations, Targets_met, Division_People/HR Management, Division_Regulatory and Legal services, Year_of_recruitment, Previous_IntraDepartmental_Movement, No_of_previous_employers, Division_Business Finance Operations, Gender_Male, Qualification",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-0add4ab41b83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# predict using the itest data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, output_margin, ntree_limit, validate_features)\u001b[0m\n\u001b[0;32m    789\u001b[0m                                                  \u001b[0moutput_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_margin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m                                                  \u001b[0mntree_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mntree_limit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m                                                  validate_features=validate_features)\n\u001b[0m\u001b[0;32m    792\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[1;31m# If output_margin is active, simply return the scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1283\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidate_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1284\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m         \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_validate_features\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1689\u001b[0m                 raise ValueError(msg.format(self.feature_names,\n\u001b[1;32m-> 1690\u001b[1;33m                                             data.feature_names))\n\u001b[0m\u001b[0;32m   1691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1692\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_split_value_histogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_pandas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20'] ['Qualification', 'Last_performance_score', 'Year_of_recruitment', 'Targets_met', 'Previous_Award', 'Training_score_average', 'Foreign_schooled', 'Past_Disciplinary_Action', 'Previous_IntraDepartmental_Movement', 'No_of_previous_employers', 'Gender_Female', 'Gender_Male', 'Division_Business Finance Operations', 'Division_Commercial Sales and Marketing', 'Division_Customer Support and Field Operations', 'Division_Information Technology and Solution Support', 'Division_Information and Strategy', 'Division_People/HR Management', 'Division_Regulatory and Legal services', 'Division_Research and Innovation', 'Division_Sourcing and Purchasing']\nexpected f17, f11, f0, f13, f2, f20, f8, f19, f9, f12, f4, f1, f15, f14, f18, f6, f7, f5, f10, f16, f3 in input data\ntraining data did not have the following fields: Past_Disciplinary_Action, Last_performance_score, Gender_Female, Division_Research and Innovation, Division_Information Technology and Solution Support, Division_Commercial Sales and Marketing, Division_Sourcing and Purchasing, Training_score_average, Previous_Award, Division_Information and Strategy, Foreign_schooled, Division_Customer Support and Field Operations, Targets_met, Division_People/HR Management, Division_Regulatory and Legal services, Year_of_recruitment, Previous_IntraDepartmental_Movement, No_of_previous_employers, Division_Business Finance Operations, Gender_Male, Qualification"
     ]
    }
   ],
   "source": [
    "# predict using the itest data\n",
    "pred = xgb.predict(itest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_Id = test.EmployeeNo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data = {'EmployeeNo':target_Id, 'Promoted_or_Not':pred})\n",
    "output.to_csv(path_or_buf = 'xgbbumiepredi.csv', index = False, quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
